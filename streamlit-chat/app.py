import streamlit as st
import json
import os
from datetime import datetime
from lmstudio_client import stream_lmstudio_response
from api_client import get_accountbook_context
from prompts import build_prompt
from chat_history import save_messages, load_messages, clear_history
from chat_logger import log_chat_interaction

# #region agent log
import os
LOG_DIR = r"c:\dev\git\ncaco97\2026\2026_idea_mvp_01\.cursor"
LOG_PATH = os.path.join(LOG_DIR, "debug.log")
def _log(session_id, run_id, hypothesis_id, location, message, data):
    try:
        os.makedirs(LOG_DIR, exist_ok=True)
        with open(LOG_PATH, "a", encoding="utf-8") as f:
            f.write(json.dumps({"sessionId": session_id, "runId": run_id, "hypothesisId": hypothesis_id, "location": location, "message": message, "data": data, "timestamp": int(datetime.now().timestamp() * 1000)}) + "\n")
            f.flush()
    except Exception as e:
        print(f"ë¡œê·¸ ì“°ê¸° ì‹¤íŒ¨: {e}, ê²½ë¡œ: {LOG_PATH}")
# #endregion

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI ê°€ê³„ë¶€ ì±„íŒ…",
    page_icon="ğŸ’¬",
    layout="centered",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': None,
        'Report a bug': None,
        'About': None
    }  # ëª¨ë“  ë©”ë‰´ í•­ëª© ì œê±°
)

# ì»¤ìŠ¤í…€ CSSë¡œ ë””ìì¸ ê°œì„ 
st.markdown("""
<style>
    /* Streamlit í—¤ë” ë©”ë‰´ ìˆ¨ê¸°ê¸° */
    #MainMenu {
        visibility: hidden !important;
        height: 0 !important;
        display: none !important;
    }
    
    header[data-testid="stHeader"] {
        visibility: hidden !important;
        height: 0 !important;
        display: none !important;
    }
    
    /* Deploy ë²„íŠ¼ ìˆ¨ê¸°ê¸° */
    .stDeployButton {
        display: none !important;
        visibility: hidden !important;
    }
    
    /* ì„¤ì • ë©”ë‰´ ë²„íŠ¼ë“¤ ìˆ¨ê¸°ê¸° */
    button[title="View app source"],
    button[title="Get help"],
    button[title="Report a bug"],
    button[title="About"],
    button[kind="header"] {
        display: none !important;
        visibility: hidden !important;
    }
    
    /* í—¤ë” ì˜ì—­ ì „ì²´ ìˆ¨ê¸°ê¸° */
    div[data-testid="stHeader"],
    section[data-testid="stHeader"] {
        display: none !important;
        visibility: hidden !important;
        height: 0 !important;
    }
    
    /* ëª¨ë“  ë ˆë²¨ì—ì„œ ìŠ¤í¬ë¡¤ ì œê±° */
    html, body {
        height: 100vh !important;
        overflow: hidden !important;
        margin: 0 !important;
        padding: 0 !important;
    }
    
    #root {
        height: 100vh !important;
        overflow: hidden !important;
    }
    
    .main {
        height: 100vh !important;
        overflow: hidden !important;
    }
    
    /* ë©”ì¸ ì»¨í…Œì´ë„ˆ - flexbox ë ˆì´ì•„ì›ƒ, ì •í™•í•œ ë†’ì´ */
    .main .block-container {
        padding-top: 0.5rem !important;
        padding-bottom: 0.5rem !important;
        max-width: 100% !important;
        width: 100% !important;
        height: 100vh !important;
        display: flex !important;
        flex-direction: column !important;
        overflow: hidden !important;
        margin: 0 auto !important;
    }
    
    /* ì‚¬ì´ë“œë°”ê°€ ìˆì„ ë•Œ ë©”ì¸ ì˜ì—­ ì¡°ì • */
    .main:has([data-testid="stSidebar"]) .block-container {
        padding-left: 1rem !important;
    }
    
    /* ì œëª©ê³¼ ìº¡ì…˜ ê³ ì • */
    h1 {
        margin-bottom: 0.25rem !important;
        flex-shrink: 0 !important;
        font-size: 1.5rem !important;
    }
    
    .stCaption {
        flex-shrink: 0 !important;
        margin-bottom: 0.75rem !important;
        font-size: 0.875rem !important;
    }
    
    /* ì±„íŒ… ë©”ì‹œì§€ ì˜ì—­ - ìŠ¤í¬ë¡¤ ê°€ëŠ¥, ë‚˜ë¨¸ì§€ ê³µê°„ ì°¨ì§€ */
    .main .block-container > div[data-testid="stVerticalBlock"]:not(:has([data-testid="stChatInput"])) {
        flex: 1 1 auto !important;
        overflow-y: auto !important;
        overflow-x: hidden !important;
        min-height: 0 !important;
        max-height: none !important;
        padding-right: 0.5rem !important;
    }
    
    /* ì…ë ¥ í•„ë“œê°€ ìˆëŠ” ë¸”ë¡ - ê³ ì • */
    .main .block-container > div[data-testid="stVerticalBlock"]:has([data-testid="stChatInput"]) {
        flex: 0 0 auto !important;
        overflow: visible !important;
        max-height: none !important;
        padding-top: 0.5rem !important;
        border-top: 1px solid #e5e7eb !important;
        background-color: white !important;
        position: sticky !important;
        bottom: 0 !important;
        z-index: 100 !important;
    }
    
    /* ì±„íŒ… ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    [data-testid="stChatMessage"] {
        padding: 0.75rem !important;
        margin-bottom: 0.5rem !important;
        border-radius: 0.5rem !important;
    }
    
    /* ì‚¬ìš©ì ë©”ì‹œì§€ */
    [data-testid="stChatMessage"]:has([data-testid="stChatMessageUser"]) {
        background-color: #f3f4f6 !important;
    }
    
    /* AI ë©”ì‹œì§€ */
    [data-testid="stChatMessage"]:has([data-testid="stChatMessageAssistant"]) {
        background-color: #ffffff !important;
        border: 1px solid #e5e7eb !important;
    }
    
    /* Info ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    [data-testid="stMarkdownContainer"] {
        padding: 0.5rem 0 !important;
    }
    
    /* ìŠ¤í¬ë¡¤ë°” ìŠ¤íƒ€ì¼ */
    .main .block-container > div[data-testid="stVerticalBlock"]::-webkit-scrollbar {
        width: 6px !important;
    }
    
    .main .block-container > div[data-testid="stVerticalBlock"]::-webkit-scrollbar-track {
        background: #f1f1f1 !important;
        border-radius: 3px !important;
    }
    
    .main .block-container > div[data-testid="stVerticalBlock"]::-webkit-scrollbar-thumb {
        background: #c1c1c1 !important;
        border-radius: 3px !important;
    }
    
    .main .block-container > div[data-testid="stVerticalBlock"]::-webkit-scrollbar-thumb:hover {
        background: #a8a8a8 !important;
    }
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    # ì €ì¥ëœ íˆìŠ¤í† ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
    saved_messages = load_messages()
    st.session_state.messages = saved_messages if saved_messages else []

if "lmstudio_url" not in st.session_state:
    st.session_state.lmstudio_url = "http://127.0.0.1:1234"

if "model" not in st.session_state:
    st.session_state.model = ""

if "history_loaded" not in st.session_state:
    st.session_state.history_loaded = True

if "show_logs" not in st.session_state:
    st.session_state.show_logs = False

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")
    st.session_state.lmstudio_url = st.text_input(
        "LM Studio URL",
        value=st.session_state.lmstudio_url,
        help="LM Studio ì„œë²„ ì£¼ì†Œ"
    )
    st.session_state.model = st.text_input(
        "ëª¨ë¸ ì´ë¦„",
        value=st.session_state.model,
        help="LM Studioì—ì„œ ë¡œë“œí•œ ëª¨ë¸ ì´ë¦„"
    )
    
    st.divider()
    
    if st.button("ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.messages = []
        clear_history()
        st.success("ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.rerun()
    
    st.divider()
    
    # íˆìŠ¤í† ë¦¬ ì •ë³´
    if st.session_state.messages:
        st.caption(f"ğŸ’¾ ì €ì¥ëœ ëŒ€í™”: {len(st.session_state.messages)}ê°œ ë©”ì‹œì§€")
        if st.button("íˆìŠ¤í† ë¦¬ ì‚­ì œ", use_container_width=True, type="secondary"):
            clear_history()
            st.session_state.messages = []
            st.success("íˆìŠ¤í† ë¦¬ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.rerun()
    
    st.divider()
    
    # ì±„íŒ… ë¡œê·¸ ì •ë³´
    from chat_logger import get_recent_logs
    recent_logs = get_recent_logs(limit=10)
    if recent_logs:
        st.caption(f"ğŸ“ ì±„íŒ… ë¡œê·¸: {len(recent_logs)}ê°œ ê¸°ë¡")
        if st.button("ë¡œê·¸ ë³´ê¸°", use_container_width=True, type="secondary"):
            st.session_state.show_logs = not st.session_state.get("show_logs", False)
    
    if st.session_state.get("show_logs", False):
        st.markdown("### ìµœê·¼ ì±„íŒ… ë¡œê·¸")
        for i, log in enumerate(reversed(recent_logs[-5:])):  # ìµœê·¼ 5ê°œë§Œ í‘œì‹œ
            with st.expander(f"ë¡œê·¸ {i+1} - {log.get('timestamp', '')[:19]}"):
                st.text("ì‚¬ìš©ì ì…ë ¥:")
                st.code(log.get('user_input', ''), language='text')
                st.text("ëª¨ë¸ í”„ë¡¬í”„íŠ¸:")
                st.code(log.get('model_prompt', '')[:500] + '...' if len(log.get('model_prompt', '')) > 500 else log.get('model_prompt', ''), language='text')
                st.text("ëª¨ë¸ ì‘ë‹µ:")
                st.code(log.get('model_response', '')[:500] + '...' if len(log.get('model_response', '')) > 500 else log.get('model_response', ''), language='text')
                if log.get('error'):
                    st.error(f"ì˜¤ë¥˜: {log.get('error')}")

# ë©”ì¸ ì˜ì—­
st.title("ğŸ’¬ AI ê°€ê³„ë¶€ ì±„íŒ…")
st.caption("ê°€ê³„ë¶€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸í•˜ê³  ë‹µë³€ì„ ë°›ì•„ë³´ì„¸ìš”.")

# ì±„íŒ… ë©”ì‹œì§€ ì˜ì—­ (ìŠ¤í¬ë¡¤ ê°€ëŠ¥)
if st.session_state.messages:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
else:
    st.info("ğŸ‘‹ ì•ˆë…•í•˜ì„¸ìš”! ê°€ê³„ë¶€ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”.")
    st.markdown("""
    **ì˜ˆì‹œ ì§ˆë¬¸:**
    - ì´ë²ˆ ë‹¬ ì§€ì¶œì€ ì–¼ë§ˆì¸ê°€ìš”?
    - ê°€ì¥ ë§ì´ ì§€ì¶œí•œ ì¹´í…Œê³ ë¦¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
    - ìµœê·¼ ê±°ë˜ ë‚´ì—­ì„ ì•Œë ¤ì£¼ì„¸ìš”.
    - ì§€ì¶œ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
    """)

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # #region agent log
    _log("debug-session", "run1", "D", "app.py:chat_input", "ì‚¬ìš©ì ì…ë ¥ ìˆ˜ì‹ ", {"prompt": prompt[:100]})
    # #endregion
    
                # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    save_messages(st.session_state.messages)
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                # ê°€ê³„ë¶€ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                # #region agent log
                _log("debug-session", "run1", "D", "app.py:chat_input", "ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘", {})
                # #endregion
                context = get_accountbook_context(user_question=prompt)
                
                # #region agent log
                _log("debug-session", "run1", "D", "app.py:chat_input", "ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ", {"context_length": len(context)})
                # #endregion
                
                # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                full_prompt = build_prompt(prompt, context)
                
                # #region agent log
                _log("debug-session", "run1", "D", "app.py:chat_input", "í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì™„ë£Œ", {"full_prompt_length": len(full_prompt), "lmstudio_url": st.session_state.lmstudio_url, "model": st.session_state.model})
                # #endregion
                
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
                response_placeholder = st.empty()
                full_response = ""
                
                # #region agent log
                _log("debug-session", "run1", "D", "app.py:chat_input", "ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘", {})
                # #endregion
                
                for chunk in stream_lmstudio_response(
                    st.session_state.lmstudio_url,
                    st.session_state.model,
                    full_prompt
                ):
                    if chunk:
                        full_response += chunk
                        response_placeholder.markdown(full_response + "â–Œ")
                
                # #region agent log
                _log("debug-session", "run1", "D", "app.py:chat_input", "ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ", {"response_length": len(full_response)})
                # #endregion
                
                # ìµœì¢… ì‘ë‹µ í‘œì‹œ
                response_placeholder.markdown(full_response)
                
                # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response
                })
                
                # íˆìŠ¤í† ë¦¬ ì €ì¥
                save_messages(st.session_state.messages)
                
                # ì±„íŒ… ë¡œê·¸ ê¸°ë¡
                log_chat_interaction(
                    user_input=prompt,
                    model_prompt=full_prompt,
                    model_response=full_response,
                    model_name=st.session_state.model,
                    lmstudio_url=st.session_state.lmstudio_url,
                    context_length=len(context),
                    response_length=len(full_response)
                )
                
            except Exception as e:
                # #region agent log
                _log("debug-session", "run1", "D", "app.py:chat_input", "ì˜¤ë¥˜ ë°œìƒ", {"error": str(e), "error_type": type(e).__name__})
                # #endregion
                error_message = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_message)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_message
                })
                
                # ì˜¤ë¥˜ ë¡œê·¸ ê¸°ë¡
                try:
                    context = get_accountbook_context()
                    full_prompt = build_prompt(prompt, context)
                    log_chat_interaction(
                        user_input=prompt,
                        model_prompt=full_prompt,
                        model_response="",
                        model_name=st.session_state.model,
                        lmstudio_url=st.session_state.lmstudio_url,
                        error=str(e)
                    )
                except:
                    pass
